{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import os\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import nltk\n",
    "import re\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "from  sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "from itertools import chain\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_data_path='../original_data/'\n",
    "saved_data_path='../saved_data/'\n",
    "plots_path='../plots/'\n",
    "\n",
    "codes_path='../codes/'\n",
    "if not codes_path in sys.path: sys.path.append(codes_path)\n",
    "import chooseFeature\n",
    "import classification_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '../saved_data/modified_text_with_3500_clusters/train_modified_1000.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ef96f321377a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0minputFName\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msaved_data_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'modified_text_with_3500_clusters/train_modified_1000.json'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0minJSON\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputFName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minJSON\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '../saved_data/modified_text_with_3500_clusters/train_modified_1000.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "inputFName=saved_data_path+'modified_text_with_3500_clusters/train_modified_1000.json'\n",
    "inJSON = json.load(open(inputFName, \"r\"))\n",
    "train=pd.DataFrame(inJSON)\n",
    "\n",
    "\n",
    "inputFName=saved_data_path+'modified_text_with_3500_clusters/test_modified_1000.json'\n",
    "inJSON = json.load(open(inputFName, \"r\"))\n",
    "test=pd.DataFrame(inJSON)\n",
    "\n",
    "train['review_cleaned']=train['review'].map(lambda x: \" \".join(x))\n",
    "test['review_cleaned']=test['review'].map(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read data from files\n",
    "train = pd.read_csv(original_data_path+'labeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3 )\n",
    "test = pd.read_csv(original_data_path+'testData.tsv', header=0, delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import text_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train done\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "train['review']=train['review'].map(lambda x: BeautifulSoup(x).get_text().lower().replace('\\\"',''))\n",
    "print('train done')\n",
    "\n",
    "test['review']=test['review'].map(lambda x: BeautifulSoup(x).get_text().lower().replace('\\\"',''))\n",
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train done\n",
      "test done\n"
     ]
    }
   ],
   "source": [
    "train['review_cleaned']=train['review'].map(text_processing.prepareWords)\n",
    "print('train done')\n",
    "test['review_cleaned']=test['review'].map(text_processing.prepareWords)\n",
    "print('test done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outJSON=test\n",
    "outputFName=saved_data_path+'testSet.pkl'\n",
    "with open(outputFName, 'wb') as fid:\n",
    "    cPickle.dump(outJSON, fid)  \n",
    "    \n",
    "outJSON=train\n",
    "outputFName=saved_data_path+'train_all_Set.pkl'\n",
    "with open(outputFName, 'wb') as fid:\n",
    "    cPickle.dump(outJSON, fid)  \n",
    "\n",
    "from sklearn import cross_validation\n",
    "kf = cross_validation.KFold(train.shape[0], n_folds=5,shuffle=True)\n",
    "for j,(train_idx, test_idx) in enumerate(kf):\n",
    "    \n",
    "    outJSON=train.ix[train_idx]\n",
    "    outputFName=saved_data_path+'trainingSet_%i.pkl'%(j) \n",
    "    with open(outputFName, 'wb') as fid:\n",
    "        cPickle.dump(outJSON, fid)  \n",
    "\n",
    "    outJSON=train.ix[test_idx]\n",
    "    outputFName=saved_data_path+'validationSet_%i.pkl'%(j)\n",
    "    with open(outputFName, 'wb') as fid:\n",
    "        cPickle.dump(outJSON, fid)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# load saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputFName=saved_data_path+'train_all_Set.pkl'\n",
    "with open(inputFName,'rb') as fp:\n",
    "    train=cPickle.load(fp)\n",
    "    \n",
    "inputFName=saved_data_path+'testSet.pkl'\n",
    "with open(inputFName,'rb') as fp:\n",
    "    test=cPickle.load(fp) \n",
    "    \n",
    "train_test=pd.DataFrame(pd.concat([train['review_cleaned'],test['review_cleaned']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check bigram and unigram numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_words_train=train['review_cleaned'].map(lambda x:x.split())\n",
    "all_words_test=test['review_cleaned'].map(lambda x:x.split())\n",
    "all_words=all_words_test+all_words_train\n",
    "\n",
    "bigrams_all=[]\n",
    "for elt in all_words:\n",
    "    bigrams_all+=list(nltk.bigrams(elt))\n",
    "    \n",
    "bigrams_all_fd = FreqDist(bigrams_all)\n",
    "\n",
    "all_words_train=list(chain.from_iterable(all_words_train))\n",
    "all_words_test=list(chain.from_iterable(all_words_test))\n",
    "\n",
    "all_words=all_words_train+all_words_test\n",
    "all_words_test,all_words_train=None,None\n",
    "\n",
    "all_words_fd = FreqDist(all_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_words > 0:103066,bigrams_all>0: 3004466 \n"
     ]
    }
   ],
   "source": [
    "print('all_words > 0:%i,bigrams_all>0: %i '%(len(set(all_words)),len(set(bigrams_all))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams_cut_off_2:48430,bigrams_cut_off_2: 315827 \n"
     ]
    }
   ],
   "source": [
    "bigrams_cut_off_2=list(set([k for k, v in bigrams_all_fd.iteritems()  if v>2]))\n",
    "unigrams_cut_off_2=list(set([k for k, v in all_words_fd.iteritems()  if v>2]))\n",
    "print('unigrams_cut_off_2:%i,bigrams_cut_off_2: %i '%(len(set(unigrams_cut_off_2)),len(set(bigrams_cut_off_2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams_cut_off_5:34109,bigrams_cut_off_5: 116178 \n"
     ]
    }
   ],
   "source": [
    "bigrams_cut_off_5=list(set([k for k, v in bigrams_all_fd.iteritems()  if v>5]))\n",
    "unigrams_cut_off_5=list(set([k for k, v in all_words_fd.iteritems()  if v>5]))\n",
    "print('unigrams_cut_off_5:%i,bigrams_cut_off_5: %i '%(len(set(unigrams_cut_off_5)),len(set(bigrams_cut_off_5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words,bigrams_all,unigrams_cut_off_2,bigrams_cut_off_2,unigrams_cut_off_5,bigrams_cut_off_5=\\\n",
    "None,None,None,None,None,None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create tfidf models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidfer_bigrams_cut_off_2 = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words =None)\n",
    "\n",
    "tfidfer_bigrams_cut_off_2.fit(list(train_test['review_cleaned']))\n",
    "X_bigrams_cut_off_2=tfidfer_bigrams_cut_off_2.transform(train_test['review_cleaned'])\n",
    "tfidfer_bigrams_cut_off_2=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfer_bigrams_cut_off_5 = TfidfVectorizer(min_df=6,  max_features=None, \n",
    "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words =None)\n",
    "\n",
    "\n",
    "tfidfer_bigrams_cut_off_5.fit(list(train_test['review_cleaned']))\n",
    "X_bigrams_cut_off_5=tfidfer_bigrams_cut_off_5.transform(train_test['review_cleaned'])\n",
    "tfidfer_bigrams_cut_off_5=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfer_bigrams_cut_off_10 = TfidfVectorizer(min_df=11,  max_features=None, \n",
    "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words =None)\n",
    "\n",
    "\n",
    "tfidfer_bigrams_cut_off_10.fit(list(train_test['review_cleaned']))\n",
    "X_bigrams_cut_off_10=tfidfer_bigrams_cut_off_10.transform(train_test['review_cleaned'])\n",
    "tfidfer_bigrams_cut_off_10=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidfer_unigrams_cut_off_2 = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 1), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words =None)\n",
    "\n",
    "tfidfer_unigrams_cut_off_2.fit(list(train_test['review_cleaned']))\n",
    "X_unigrams_cut_off_2=tfidfer_unigrams_cut_off_2.transform(train_test['review_cleaned'])\n",
    "tfidfer_unigrams_cut_off_2=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidfer_trigrams_cut_off_2 = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words =None)\n",
    "\n",
    "tfidfer_trigrams_cut_off_2.fit(list(train_test['review_cleaned']))\n",
    "X_trigrams_cut_off_2=tfidfer_trigrams_cut_off_2.transform(train_test['review_cleaned'])\n",
    "tfidfer_trigrams_cut_off_2=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfer_trigrams_cut_off_10 = TfidfVectorizer(min_df=11,  max_features=None, \n",
    "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words =None)\n",
    "\n",
    "tfidfer_trigrams_cut_off_10.fit(list(train_test['review_cleaned']))\n",
    "X_trigrams_cut_off_10=tfidfer_trigrams_cut_off_10.transform(train_test['review_cleaned'])\n",
    "tfidfer_trigrams_cut_off_10=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfer_bigrams_cut_off_40 = TfidfVectorizer(min_df=41,  max_features=None, \n",
    "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words =None)\n",
    "\n",
    "\n",
    "tfidfer_bigrams_cut_off_40.fit(list(train_test['review_cleaned']))\n",
    "X_bigrams_cut_off_40=tfidfer_bigrams_cut_off_40.transform(train_test['review_cleaned'])\n",
    "tfidfer_bigrams_cut_off_40=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfer_bigrams_cut_off_200 = TfidfVectorizer(min_df=201,  max_features=None, \n",
    "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words =None)\n",
    "\n",
    "\n",
    "tfidfer_bigrams_cut_off_200.fit(list(train_test['review_cleaned']))\n",
    "X_bigrams_cut_off_200=tfidfer_bigrams_cut_off_200.transform(train_test['review_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45657, 144308, 353082, 73048, 19026, 4228, 417922, 77427)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_unigrams_cut_off_2.shape[1],\\\n",
    "X_bigrams_cut_off_5.shape[1],\\\n",
    "X_bigrams_cut_off_2.shape[1],\\\n",
    "X_bigrams_cut_off_10.shape[1],\\\n",
    "X_bigrams_cut_off_40.shape[1],\\\n",
    "X_bigrams_cut_off_200.shape[1],\\\n",
    "X_trigrams_cut_off_2.shape[1],\\\n",
    "X_trigrams_cut_off_10.shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61507999999999996"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=chooseFeature.chooseFeature()\n",
    "X_train_array=X_bigrams_cut_off_200.toarray()[:train_len]\n",
    "type(X_train_array)\n",
    "clf.fit(X_train_array,y)\n",
    "clf.score(X_train_array,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams_cut_off_5\n",
      "\n",
      "validation set: 0\n",
      "validation set: 1\n",
      "validation set: 2\n",
      "validation set: 3\n",
      "validation set: 4\n",
      "\n",
      "cross-validation accuracy:0.898\n",
      "training accuracy:1.000\n",
      "cross-validation AUC:0.964\n",
      "training AUC:1.000\n",
      "best regularization parameter for cross-validation accuracy:14.00\n",
      "best regularization parameter for AUC:14.00\n",
      "\n",
      "trigrams_cut_off_2\n",
      "\n",
      "validation set: 0\n",
      "validation set: 1\n",
      "validation set: 2\n",
      "validation set: 3\n",
      "validation set: 4\n",
      "\n",
      "cross-validation accuracy:0.903\n",
      "training accuracy:1.000\n",
      "cross-validation AUC:0.966\n",
      "training AUC:1.000\n",
      "best regularization parameter for cross-validation accuracy:500.00\n",
      "best regularization parameter for AUC:75.00\n",
      "\n",
      "bigrams_cut_off_2\n",
      "\n",
      "validation set: 0\n",
      "validation set: 1\n",
      "validation set: 2\n",
      "validation set: 3\n",
      "validation set: 4\n",
      "\n",
      "cross-validation accuracy:0.903\n",
      "training accuracy:1.000\n",
      "cross-validation AUC:0.966\n",
      "training AUC:1.000\n",
      "best regularization parameter for cross-validation accuracy:75.00\n",
      "best regularization parameter for AUC:500.00\n",
      "\n",
      "unigrams_cut_off_2\n",
      "\n",
      "validation set: 0\n",
      "validation set: 1\n",
      "validation set: 2\n",
      "validation set: 3\n",
      "validation set: 4\n",
      "\n",
      "cross-validation accuracy:0.892\n",
      "training accuracy:1.000\n",
      "cross-validation AUC:0.958\n",
      "training AUC:1.000\n",
      "best regularization parameter for cross-validation accuracy:4.00\n",
      "best regularization parameter for AUC:4.00\n",
      "\n",
      "bigrams_cut_off_10\n",
      "\n",
      "validation set: 0\n",
      "validation set: 1\n",
      "validation set: 2\n",
      "validation set: 3\n",
      "validation set: 4\n",
      "\n",
      "cross-validation accuracy:0.900\n",
      "training accuracy:1.000\n",
      "cross-validation AUC:0.964\n",
      "training AUC:1.000\n",
      "best regularization parameter for cross-validation accuracy:6.00\n",
      "best regularization parameter for AUC:6.00\n",
      "\n",
      "bigrams_cut_off_40\n",
      "\n",
      "validation set: 0\n",
      "validation set: 1\n",
      "validation set: 2\n",
      "validation set: 3\n",
      "validation set: 4\n",
      "\n",
      "cross-validation accuracy:0.901\n",
      "training accuracy:1.000\n",
      "cross-validation AUC:0.965\n",
      "training AUC:1.000\n",
      "best regularization parameter for cross-validation accuracy:4.00\n",
      "best regularization parameter for AUC:3.00\n",
      "\n",
      "trigrams_cut_off_10\n",
      "\n",
      "validation set: 0\n",
      "validation set: 1\n",
      "validation set: 2\n",
      "validation set: 3\n",
      "validation set: 4\n",
      "\n",
      "cross-validation accuracy:0.905\n",
      "training accuracy:1.000\n",
      "cross-validation AUC:0.965\n",
      "training AUC:1.000\n",
      "best regularization parameter for cross-validation accuracy:23.00\n",
      "best regularization parameter for AUC:7.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y=train['sentiment']\n",
    "train_len=train.shape[0]\n",
    "\n",
    "clf = LogisticRegression(penalty='l2', dual=False, tol=0.0001, \n",
    "                         C=1, fit_intercept=True, intercept_scaling=1.0, \n",
    "                         class_weight=None, random_state=None)\n",
    "params=[0.1,0.5,1,2,3,4,5,6,7,14,18,20,21,22,23,30,40,50,75,100,500]\n",
    "\n",
    "\n",
    "\n",
    "##  bigrams_cut_off_5\n",
    "print('bigrams_cut_off_5\\n')\n",
    "X_all=X_bigrams_cut_off_5\n",
    "filePath=plots_path+'logistic_bigrams_cut_off_5.png'\n",
    "title='Logistic Regression'\n",
    "title+=' 5-fold cross-validation'\n",
    "title+='\\nbigrams with frequency cut-off 5'\n",
    "xlab='regularization (logistic regression C parameter)'\n",
    "classification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n",
    "\n",
    "##  trigrams_cut_off_2\n",
    "print('trigrams_cut_off_2\\n')\n",
    "X_all=X_trigrams_cut_off_2\n",
    "filePath=plots_path+'logistic_trigrams_cut_off_2.png'\n",
    "title='Logistic Regression'\n",
    "title+=' 5-fold cross-validation'\n",
    "title+='\\ntrigrams with frequency cut-off 2'\n",
    "xlab='regularization (logistic regression C parameter)'\n",
    "classification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n",
    "\n",
    "##  bigrams_cut_off_2\n",
    "print('bigrams_cut_off_2\\n')\n",
    "X_all=X_bigrams_cut_off_2\n",
    "filePath=plots_path+'logistic_bigrams_cut_off_2.png'\n",
    "title='Logistic Regression'\n",
    "title+=' 5-fold cross-validation'\n",
    "title+='\\nbigrams with frequency cut-off 2'\n",
    "xlab='regularization (logistic regression C parameter)'\n",
    "classification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n",
    "\n",
    "\n",
    "##  unigrams_cut_off_2\n",
    "print('unigrams_cut_off_2\\n')\n",
    "X_all=X_unigrams_cut_off_2\n",
    "filePath=plots_path+'logistic_unigrams_cut_off_2.png'\n",
    "title='Logistic Regression'\n",
    "title+=' 5-fold cross-validation'\n",
    "title+='\\nunigrams with frequency cut-off 2'\n",
    "xlab='regularization (logistic regression C parameter)'\n",
    "classification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n",
    "\n",
    "\n",
    "##  bigrams_cut_off_10\n",
    "print('bigrams_cut_off_10\\n')\n",
    "X_all=X_bigrams_cut_off_10\n",
    "filePath=plots_path+'logistic_bigrams_cut_off_10.png'\n",
    "title='Logistic Regression'\n",
    "title+=' 5-fold cross-validation'\n",
    "title+='\\nbigrams with frequency cut-off 10'\n",
    "xlab='regularization (logistic regression C parameter)'\n",
    "classification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n",
    "\n",
    "##  bigrams_cut_off_40\n",
    "print('bigrams_cut_off_40\\n')\n",
    "X_all=X_bigrams_cut_off_40\n",
    "filePath=plots_path+'logistic_bigrams_cut_off_40.png'\n",
    "title='Logistic Regression'\n",
    "title+=' 5-fold cross-validation'\n",
    "title+='\\nbigrams with frequency cut-off 40'\n",
    "xlab='regularization (logistic regression C parameter)'\n",
    "classification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n",
    "\n",
    "\n",
    "##  bigrams_cut_off_200\n",
    "print('bigrams_cut_off_200\\n')\n",
    "X_all=X_bigrams_cut_off_200\n",
    "filePath=plots_path+'logistic_bigrams_cut_off_200.png'\n",
    "title='Logistic Regression'\n",
    "title+=' 5-fold cross-validation'\n",
    "title+='\\nbigrams with frequency cut-off 200'\n",
    "xlab='regularization (logistic regression C parameter)'\n",
    "classification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n",
    "\n",
    "\n",
    "\n",
    "##  trigrams_cut_off_10\n",
    "print('trigrams_cut_off_10\\n')\n",
    "X_all=X_trigrams_cut_off_10\n",
    "filePath=plots_path+'logistic_trigrams_cut_off_10.png'\n",
    "title='Logistic Regression'\n",
    "title+=' 5-fold cross-validation'\n",
    "title+='\\ntrigrams with frequency cut-off 10'\n",
    "xlab='regularization (logistic regression C parameter)'\n",
    "classification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='l2', dual=False, tol=0.0001, \n",
    "                         C=1, fit_intercept=True, intercept_scaling=1.0, \n",
    "                         class_weight=None, random_state=None)\n",
    "\n",
    "clf.set_params(C=100)\n",
    "X_all=X_bigrams_cut_off_2\n",
    "X_all_train=X_all[:train_len]  \n",
    "X_all_test=X_all[train_len:]  \n",
    "\n",
    "\n",
    "clf.fit(X_all_train,y)\n",
    "\n",
    "predicted=clf.predict_proba(X_all_test)[:,1]\n",
    "\n",
    "inputFName=saved_data_path+'testSet.pkl'\n",
    "with open(inputFName,'rb') as fp:\n",
    "    test_current=cPickle.load(fp)\n",
    "\n",
    "test_current['sentiment']=predicted\n",
    "test_current['id']=test_current['id'].map(lambda x:x.replace('\"', '').strip())\n",
    "test_current=test_current[['id','sentiment']]\n",
    "test_current.to_csv(saved_data_path+'logisticRegression_bigrams_cut_off_2_submission.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
