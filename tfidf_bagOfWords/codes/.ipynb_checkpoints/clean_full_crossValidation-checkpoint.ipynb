{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import os\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import nltk\n",
    "import re\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "from  sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "from itertools import chain\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_data_path='../original_data/'\n",
    "saved_data_path='../saved_data/'\n",
    "plots_path='../plots/'\n",
    "\n",
    "codes_path='../codes/'\n",
    "if not codes_path in sys.path: sys.path.append(codes_path)\n",
    "import chooseFeature\n",
    "import classification_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read data from files\n",
    "train = pd.read_csv(original_data_path+'labeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3 )\n",
    "test = pd.read_csv(original_data_path+'testData.tsv', header=0, delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import text_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train done\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "train['review']=train['review'].map(lambda x: BeautifulSoup(x).get_text().lower().replace('\\\"',''))\n",
    "print('train done')\n",
    "\n",
    "test['review']=test['review'].map(lambda x: BeautifulSoup(x).get_text().lower().replace('\\\"',''))\n",
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train done\n"
     ]
    }
   ],
   "source": [
    "train['review_cleaned']=train['review'].map(text_processing.prepareWords)\n",
    "print('train done')\n",
    "test['review_cleaned']=test['review'].map(text_processing.prepareWords)\n",
    "print('test done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outJSON=test\n",
    "outputFName=saved_data_path+'testSet.pkl'\n",
    "with open(outputFName, 'wb') as fid:\n",
    "    cPickle.dump(outJSON, fid)  \n",
    "    \n",
    "outJSON=train\n",
    "outputFName=saved_data_path+'train_all_Set.pkl'\n",
    "with open(outputFName, 'wb') as fid:\n",
    "    cPickle.dump(outJSON, fid)  \n",
    "\n",
    "\n",
    "kf = cross_validation.KFold(train.shape[0], n_folds=5,shuffle=True)\n",
    "for j,(train_idx, test_idx) in enumerate(kf):\n",
    "    \n",
    "    outJSON=train.ix[train_idx]\n",
    "    outputFName=saved_data_path+'trainingSet_%i.pkl'%(j) \n",
    "    with open(outputFName, 'wb') as fid:\n",
    "        cPickle.dump(outJSON, fid)  \n",
    "\n",
    "    outJSON=train.ix[test_idx]\n",
    "    outputFName=saved_data_path+'validationSet_%i.pkl'%(j)\n",
    "    with open(outputFName, 'wb') as fid:\n",
    "        cPickle.dump(outJSON, fid)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# load saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputFName=saved_data_path+'train_all_Set.pkl'\n",
    "with open(inputFName,'rb') as fp:\n",
    "    train=cPickle.load(fp)\n",
    "    \n",
    "inputFName=saved_data_path+'testSet.pkl'\n",
    "with open(inputFName,'rb') as fp:\n",
    "    test=cPickle.load(fp) \n",
    "    \n",
    "train_test=pd.DataFrame(pd.concat([train['review_cleaned'],test['review_cleaned']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create tfidf models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create unigram and bigram sets, with different cut-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_words_train=train['review_cleaned'].map(lambda x:x.split())\n",
    "all_words_test=test['review_cleaned'].map(lambda x:x.split())\n",
    "all_words=all_words_test+all_words_train\n",
    "\n",
    "bigrams_all=[]\n",
    "for elt in all_words:\n",
    "    bigrams_all+=list(nltk.bigrams(elt))\n",
    "    \n",
    "bigrams_all_fd = FreqDist(bigrams_all)\n",
    "\n",
    "all_words_train=list(chain.from_iterable(all_words_train))\n",
    "all_words_test=list(chain.from_iterable(all_words_test))\n",
    "\n",
    "all_words=all_words_train+all_words_test\n",
    "all_words_test,all_words_train=None,None\n",
    "\n",
    "all_words_fd = FreqDist(all_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_words > 0:103066,bigrams_all>0: 3004466 \n"
     ]
    }
   ],
   "source": [
    "print('all_words > 0:%i,bigrams_all>0: %i '%(len(set(all_words)),len(set(bigrams_all))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams_cut_off_2:48430,bigrams_cut_off_2: 315827 \n"
     ]
    }
   ],
   "source": [
    "bigrams_cut_off_2=list(set([k for k, v in bigrams_all_fd.iteritems()  if v>2]))\n",
    "unigrams_cut_off_2=list(set([k for k, v in all_words_fd.iteritems()  if v>2]))\n",
    "print('unigrams_cut_off_2:%i,bigrams_cut_off_2: %i '%(len(set(unigrams_cut_off_2)),len(set(bigrams_cut_off_2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams_cut_off_5:34109,bigrams_cut_off_5: 116178 \n"
     ]
    }
   ],
   "source": [
    "bigrams_cut_off_5=list(set([k for k, v in bigrams_all_fd.iteritems()  if v>5]))\n",
    "unigrams_cut_off_5=list(set([k for k, v in all_words_fd.iteritems()  if v>5]))\n",
    "print('unigrams_cut_off_5:%i,bigrams_cut_off_5: %i '%(len(set(unigrams_cut_off_5)),len(set(bigrams_cut_off_5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, charset=None,\n",
       "        charset_error=None, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 2), norm=u'l2', preprocessor=None, smooth_idf=1,\n",
       "        stop_words=None, strip_accents='unicode', sublinear_tf=1,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=1,\n",
       "        vocabulary=[u'fawn', u'raining', u'nunnery', u'sonja', u'NEGpain', u'circuitry', u'burgade', u'gavan', u'hanging', u'woody', u'NEGpaid', u'comically', u'localized', u'houyhnhnm', u'disobeying', u'hypnotise', u'scola', u'scold', u'originality', u'hallucinate', u'rickman', u'symbologist', u'brodsk', u...el'), (u'turn', u'table'), (u'NEGrecommend', u'one'), (u'NEGfilm', u'ever'), (u'surprise', u'plot')])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfer_bigrams_cut_off_2 = TfidfVectorizer(min_df=2,\n",
    "        strip_accents='unicode', analyzer='word',\n",
    "        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words =None,vocabulary=unigrams_cut_off_2+bigrams_cut_off_2)\n",
    "tfidfer_bigrams_cut_off_2.fit(list(train_test['review_cleaned']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, charset=None,\n",
       "        charset_error=None, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 2), norm=u'l2', preprocessor=None, smooth_idf=1,\n",
       "        stop_words=None, strip_accents='unicode', sublinear_tf=1,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=1,\n",
       "        vocabulary=[u'fawn', u'sonja', u'NEGpain', u'burgade', u'hanging', u'woody', u'NEGpaid', u'comically', u'scola', u'scold', u'originality', u'rickman', u'brodsk', u'eugenics', u'appropriation', u'transvestism', u'taj', u'politician', u'screaming', u'wooded', u'grueling', u'wooden', u'wednesday', u'cr...u'dual'), (u'work', u'reason'), (u'turn', u'table'), (u'NEGrecommend', u'one'), (u'always', u'say')])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfer_bigrams_cut_off_5 = TfidfVectorizer(min_df=2,\n",
    "        strip_accents='unicode', analyzer='word',\n",
    "        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words =None,vocabulary=unigrams_cut_off_5+bigrams_cut_off_5)\n",
    "tfidfer_bigrams_cut_off_5.fit(list(train_test['review_cleaned']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, charset=None,\n",
       "        charset_error=None, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=1,\n",
       "        stop_words=None, strip_accents='unicode', sublinear_tf=1,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=1,\n",
       "        vocabulary=[u'fawn', u'sonja', u'NEGpain', u'burgade', u'hanging', u'woody', u'NEGpaid', u'comically', u'scola', u'scold', u'originality', u'rickman', u'brodsk', u'eugenics', u'appropriation', u'transvestism', u'taj', u'politician', u'screaming', u'wooded', u'grueling', u'wooden', u'wednesday', u'cr...t', u'NEGshow', u'sheri', u'cronyn', u'rotting', u'expands', u'emery', u'hardboiled', u'untalented'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfer_unigrams_cut_off_5 = TfidfVectorizer(min_df=2,\n",
    "        strip_accents='unicode', analyzer='word',\n",
    "        ngram_range=(1, 1), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words =None,vocabulary=unigrams_cut_off_5)\n",
    "tfidfer_unigrams_cut_off_5.fit(list(train_test['review_cleaned']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create X matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_bigrams_cut_off_5=tfidfer_bigrams_cut_off_5.transform(train_test['review_cleaned'])\n",
    "X_bigrams_cut_off_2=tfidfer_bigrams_cut_off_2.transform(train_test['review_cleaned'])\n",
    "X_unigrams_cut_off_5=tfidfer_unigrams_cut_off_5.transform(train_test['review_cleaned'])\n",
    "\n",
    "y=train['sentiment']\n",
    "train_len=train.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='l2', dual=False, tol=0.0001, \n",
    "                         C=1, fit_intercept=True, intercept_scaling=1.0, \n",
    "                         class_weight=None, random_state=None)\n",
    "params=[0.1,0.5,1,2,3,4,4.6,5,7,14,18,20,21,22,23,30,40,50,60,70,80,]\n",
    "\n",
    "X_all=X_bigrams_cut_off_5\n",
    "\n",
    "X_all_train=X_all[:train_len]\n",
    "X_all_test=X_all[train_len:]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation set: 0\n",
      "validation set: 1\n",
      "validation set: 2\n",
      "validation set: 3"
     ]
    }
   ],
   "source": [
    "(test_scores,train_scores,test_prob_scores,train_prob_scores)=\\\n",
    "classification_functions.getCrossValidation(X_all_train,y,clf,params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10440000000000005"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-np.max(np.mean(np.array(test_scores),axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(16.0, 10.0))\n",
    "filePath=plots_path+'foo.png'\n",
    "title='bigrams with frequency cut-off 5'\n",
    "ax=classification_functions.getValidationFigure(filePath,title,ax,params,test_scores,train_scores,test_prob_scores,train_prob_scores)\n",
    "plt.savefig(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='l2', dual=False, tol=0.0001, \n",
    "                         C=1, fit_intercept=True, intercept_scaling=1.0, \n",
    "                         class_weight=None, random_state=None)\n",
    "\n",
    "clf.set_params(C=25)\n",
    "\n",
    "clf.fit(X_train_all,y)\n",
    "\n",
    "predicted=clf.predict_proba(X_test_all)[:,1]\n",
    "\n",
    "inputFName='saved_data/testSet.pkl'\n",
    "with open(inputFName,'rb') as fp:\n",
    "    test_current=cPickle.load(fp)\n",
    "\n",
    "test_current['sentiment']=predicted\n",
    "test_current['id']=test_current['id'].map(lambda x:x.replace('\"', '').strip())\n",
    "test_current=test_current[['id','sentiment']]\n",
    "test_current.to_csv('saved_data/logisticRegression_allwords_bigrams_submission.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
