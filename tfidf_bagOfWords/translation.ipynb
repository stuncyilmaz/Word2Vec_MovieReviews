{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import os\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import nltk\n",
    "import re\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "from  sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "from itertools import chain\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_data_path='../original_data/'\n",
    "saved_data_path='../saved_data/'\n",
    "plots_path='../plots/'\n",
    "\n",
    "codes_path='../codes/'\n",
    "if not codes_path in sys.path: sys.path.append(codes_path)\n",
    "import chooseFeature\n",
    "import classification_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "inputFName=saved_data_path+'modified_text_with_3500_clusters/train_modified_3500.json'\n",
    "inJSON = json.load(open(inputFName, \"r\"))\n",
    "train=pd.DataFrame(inJSON)\n",
    "inJSON =None\n",
    "\n",
    "\n",
    "inputFName=saved_data_path+'modified_text_with_3500_clusters/test_modified_3500.json'\n",
    "inJSON = json.load(open(inputFName, \"r\"))\n",
    "test=pd.DataFrame(inJSON)\n",
    "inJSON =None\n",
    "\n",
    "train['review_cleaned']=train['review'].map(lambda x: \" \".join(x))\n",
    "test['review_cleaned']=test['review'].map(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_original=pd.read_csv(original_data_path+'labeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3 )\n",
    "train['sentiment']=train_original['sentiment']\n",
    "train_original=None\n",
    "train_test=pd.DataFrame(pd.concat([train['review_cleaned'],test['review_cleaned']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check bigram and unigram numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_words_train=train['review_cleaned'].map(lambda x:x.split())\n",
    "all_words_test=test['review_cleaned'].map(lambda x:x.split())\n",
    "all_words=all_words_test+all_words_train\n",
    "\n",
    "bigrams_all=[]\n",
    "for elt in all_words:\n",
    "    bigrams_all+=list(nltk.bigrams(elt))\n",
    "    \n",
    "bigrams_all_fd = FreqDist(bigrams_all)\n",
    "\n",
    "all_words_train=list(chain.from_iterable(all_words_train))\n",
    "all_words_test=list(chain.from_iterable(all_words_test))\n",
    "\n",
    "all_words=all_words_train+all_words_test\n",
    "all_words_test,all_words_train=None,None\n",
    "\n",
    "all_words_fd = FreqDist(all_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_words > 0:109668,bigrams_all>0: 2270292 \n"
     ]
    }
   ],
   "source": [
    "print('all_words > 0:%i,bigrams_all>0: %i '%(len(set(all_words)),len(set(bigrams_all))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams_cut_off_2:44912,bigrams_cut_off_2: 373490 \n"
     ]
    }
   ],
   "source": [
    "bigrams_cut_off_2=list(set([k for k, v in bigrams_all_fd.iteritems()  if v>2]))\n",
    "unigrams_cut_off_2=list(set([k for k, v in all_words_fd.iteritems()  if v>2]))\n",
    "print('unigrams_cut_off_2:%i,bigrams_cut_off_2: %i '%(len(set(unigrams_cut_off_2)),len(set(bigrams_cut_off_2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams_cut_off_5:26877,bigrams_cut_off_5: 147402 \n"
     ]
    }
   ],
   "source": [
    "bigrams_cut_off_5=list(set([k for k, v in bigrams_all_fd.iteritems()  if v>5]))\n",
    "unigrams_cut_off_5=list(set([k for k, v in all_words_fd.iteritems()  if v>5]))\n",
    "print('unigrams_cut_off_5:%i,bigrams_cut_off_5: %i '%(len(set(unigrams_cut_off_5)),len(set(bigrams_cut_off_5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words,bigrams_all,unigrams_cut_off_2,bigrams_cut_off_2,unigrams_cut_off_5,bigrams_cut_off_5=\\\n",
    "None,None,None,None,None,None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create tfidf models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidfer_bigrams_cut_off_2 = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words =None)\n",
    "\n",
    "tfidfer_bigrams_cut_off_2.fit(list(train_test['review_cleaned']))\n",
    "X_bigrams_cut_off_2=tfidfer_bigrams_cut_off_2.transform(train_test['review_cleaned'])\n",
    "tfidfer_bigrams_cut_off_2=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfer_bigrams_cut_off_5 = TfidfVectorizer(min_df=6,  max_features=None, \n",
    "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words =None)\n",
    "\n",
    "\n",
    "tfidfer_bigrams_cut_off_5.fit(list(train_test['review_cleaned']))\n",
    "X_bigrams_cut_off_5=tfidfer_bigrams_cut_off_5.transform(train_test['review_cleaned'])\n",
    "tfidfer_bigrams_cut_off_5=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfer_bigrams_cut_off_10 = TfidfVectorizer(min_df=11,  max_features=None, \n",
    "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words =None)\n",
    "\n",
    "\n",
    "tfidfer_bigrams_cut_off_10.fit(list(train_test['review_cleaned']))\n",
    "X_bigrams_cut_off_10=tfidfer_bigrams_cut_off_10.transform(train_test['review_cleaned'])\n",
    "tfidfer_bigrams_cut_off_10=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidfer_unigrams_cut_off_2 = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 1), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words =None)\n",
    "\n",
    "tfidfer_unigrams_cut_off_2.fit(list(train_test['review_cleaned']))\n",
    "X_unigrams_cut_off_2=tfidfer_unigrams_cut_off_2.transform(train_test['review_cleaned'])\n",
    "tfidfer_unigrams_cut_off_2=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidfer_trigrams_cut_off_2 = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words =None)\n",
    "\n",
    "tfidfer_trigrams_cut_off_2.fit(list(train_test['review_cleaned']))\n",
    "X_trigrams_cut_off_2=tfidfer_trigrams_cut_off_2.transform(train_test['review_cleaned'])\n",
    "tfidfer_trigrams_cut_off_2=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfer_trigrams_cut_off_10 = TfidfVectorizer(min_df=11,  max_features=None, \n",
    "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words =None)\n",
    "\n",
    "tfidfer_trigrams_cut_off_10.fit(list(train_test['review_cleaned']))\n",
    "X_trigrams_cut_off_10=tfidfer_trigrams_cut_off_10.transform(train_test['review_cleaned'])\n",
    "tfidfer_trigrams_cut_off_10=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfer_bigrams_cut_off_40 = TfidfVectorizer(min_df=41,  max_features=None, \n",
    "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words =None)\n",
    "\n",
    "\n",
    "tfidfer_bigrams_cut_off_40.fit(list(train_test['review_cleaned']))\n",
    "X_bigrams_cut_off_40=tfidfer_bigrams_cut_off_40.transform(train_test['review_cleaned'])\n",
    "tfidfer_bigrams_cut_off_40=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfer_bigrams_cut_off_200 = TfidfVectorizer(min_df=201,  max_features=None, \n",
    "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words =None)\n",
    "\n",
    "\n",
    "tfidfer_bigrams_cut_off_200.fit(list(train_test['review_cleaned']))\n",
    "X_bigrams_cut_off_200=tfidfer_bigrams_cut_off_200.transform(train_test['review_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41974, 168019, 407881, 78260, 15058, 4050, 521086, 90083)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_unigrams_cut_off_2.shape[1],\\\n",
    "X_bigrams_cut_off_5.shape[1],\\\n",
    "X_bigrams_cut_off_2.shape[1],\\\n",
    "X_bigrams_cut_off_10.shape[1],\\\n",
    "X_bigrams_cut_off_40.shape[1],\\\n",
    "X_bigrams_cut_off_200.shape[1],\\\n",
    "X_trigrams_cut_off_2.shape[1],\\\n",
    "X_trigrams_cut_off_10.shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams_cut_off_5\n",
      "\n",
      "validation set: 0\n",
      "validation set: 1\n",
      "validation set: 2\n",
      "validation set: 3\n",
      "validation set: 4\n",
      "\n",
      "cross-validation accuracy:0.896\n",
      "training accuracy:1.000\n",
      "cross-validation AUC:0.959\n",
      "training AUC:1.000\n",
      "best regularization parameter for cross-validation accuracy:7.00\n",
      "best regularization parameter for AUC:6.00\n",
      "\n",
      "trigrams_cut_off_2\n",
      "\n",
      "validation set: 0\n",
      "validation set: 1\n",
      "validation set: 2\n",
      "validation set: 3\n",
      "validation set: 4\n",
      "\n",
      "cross-validation accuracy:0.904\n",
      "training accuracy:1.000\n",
      "cross-validation AUC:0.963\n",
      "training AUC:1.000\n",
      "best regularization parameter for cross-validation accuracy:18.00\n",
      "best regularization parameter for AUC:50.00\n",
      "\n",
      "bigrams_cut_off_2\n",
      "\n",
      "validation set: 0\n",
      "validation set: 1\n",
      "validation set: 2\n",
      "validation set: 3\n",
      "validation set: 4\n",
      "\n",
      "cross-validation accuracy:0.902\n",
      "training accuracy:1.000\n",
      "cross-validation AUC:0.962\n",
      "training AUC:1.000\n",
      "best regularization parameter for cross-validation accuracy:100.00\n",
      "best regularization parameter for AUC:50.00\n",
      "\n",
      "unigrams_cut_off_2\n",
      "\n",
      "validation set: 0\n",
      "validation set: 1\n",
      "validation set: 2\n",
      "validation set: 3\n",
      "validation set: 4\n",
      "\n",
      "cross-validation accuracy:0.887\n",
      "training accuracy:1.000\n",
      "cross-validation AUC:0.951\n",
      "training AUC:1.000\n",
      "best regularization parameter for cross-validation accuracy:2.00\n",
      "best regularization parameter for AUC:2.00\n",
      "\n",
      "bigrams_cut_off_10\n",
      "\n",
      "validation set: 0\n",
      "validation set: 1\n",
      "validation set: 2\n",
      "validation set: 3\n",
      "validation set: 4\n",
      "\n",
      "cross-validation accuracy:0.894\n",
      "training accuracy:1.000\n",
      "cross-validation AUC:0.958\n",
      "training AUC:1.000\n",
      "best regularization parameter for cross-validation accuracy:3.00\n",
      "best regularization parameter for AUC:5.00\n",
      "\n",
      "bigrams_cut_off_40\n",
      "\n",
      "validation set: 0\n",
      "validation set: 1\n",
      "validation set: 2\n",
      "validation set: 3\n",
      "validation set: 4\n",
      "\n",
      "cross-validation accuracy:0.892\n",
      "training accuracy:1.000\n",
      "cross-validation AUC:0.958\n",
      "training AUC:1.000\n",
      "best regularization parameter for cross-validation accuracy:3.00\n",
      "best regularization parameter for AUC:3.00\n",
      "\n",
      "bigrams_cut_off_200\n",
      "\n",
      "validation set: 0\n",
      "validation set: 1\n",
      "validation set: 2\n",
      "validation set: 3\n",
      "validation set: 4\n",
      "\n",
      "cross-validation accuracy:0.883\n",
      "training accuracy:0.972\n",
      "cross-validation AUC:0.951\n",
      "training AUC:0.996\n",
      "best regularization parameter for cross-validation accuracy:1.00\n",
      "best regularization parameter for AUC:1.00\n",
      "\n",
      "trigrams_cut_off_10\n",
      "\n",
      "validation set: 0\n",
      "validation set: 1\n",
      "validation set: 2\n",
      "validation set: 3\n",
      "validation set: 4\n",
      "\n",
      "cross-validation accuracy:0.898\n",
      "training accuracy:1.000\n",
      "cross-validation AUC:0.960\n",
      "training AUC:1.000\n",
      "best regularization parameter for cross-validation accuracy:4.00\n",
      "best regularization parameter for AUC:4.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y=train['sentiment']\n",
    "train_len=train.shape[0]\n",
    "\n",
    "clf = LogisticRegression(penalty='l2', dual=False, tol=0.0001, \n",
    "                         C=1, fit_intercept=True, intercept_scaling=1.0, \n",
    "                         class_weight=None, random_state=None)\n",
    "params=[0.1,0.5,1,2,3,4,5,6,7,14,18,20,21,22,23,30,40,50,75,100,500]\n",
    "\n",
    "\n",
    "\n",
    "##  bigrams_cut_off_5\n",
    "print('bigrams_cut_off_5\\n')\n",
    "X_all=X_bigrams_cut_off_5\n",
    "filePath=plots_path+'logistic_bigrams_cut_off_5_translated3500Clusters.png'\n",
    "title='Logistic Regression'\n",
    "title+=' 5-fold cross-validation'\n",
    "title+='\\nbigrams with frequency cut-off 5'\n",
    "xlab='regularization (logistic regression C parameter)'\n",
    "classification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n",
    "\n",
    "##  trigrams_cut_off_2\n",
    "print('trigrams_cut_off_2\\n')\n",
    "X_all=X_trigrams_cut_off_2\n",
    "filePath=plots_path+'logistic_trigrams_cut_off_2_translated3500Clusters.png'\n",
    "title='Logistic Regression'\n",
    "title+=' 5-fold cross-validation'\n",
    "title+='\\ntrigrams with frequency cut-off 2'\n",
    "xlab='regularization (logistic regression C parameter)'\n",
    "classification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n",
    "\n",
    "##  bigrams_cut_off_2\n",
    "print('bigrams_cut_off_2\\n')\n",
    "X_all=X_bigrams_cut_off_2\n",
    "filePath=plots_path+'logistic_bigrams_cut_off_2_translated3500Clusters.png'\n",
    "title='Logistic Regression'\n",
    "title+=' 5-fold cross-validation'\n",
    "title+='\\nbigrams with frequency cut-off 2'\n",
    "xlab='regularization (logistic regression C parameter)'\n",
    "classification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n",
    "\n",
    "\n",
    "##  unigrams_cut_off_2\n",
    "print('unigrams_cut_off_2\\n')\n",
    "X_all=X_unigrams_cut_off_2\n",
    "filePath=plots_path+'logistic_unigrams_cut_off_2_translated3500Clusters.png'\n",
    "title='Logistic Regression'\n",
    "title+=' 5-fold cross-validation'\n",
    "title+='\\nunigrams with frequency cut-off 2'\n",
    "xlab='regularization (logistic regression C parameter)'\n",
    "classification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n",
    "\n",
    "\n",
    "##  bigrams_cut_off_10\n",
    "print('bigrams_cut_off_10\\n')\n",
    "X_all=X_bigrams_cut_off_10\n",
    "filePath=plots_path+'logistic_bigrams_cut_off_10_translated3500Clusters.png'\n",
    "title='Logistic Regression'\n",
    "title+=' 5-fold cross-validation'\n",
    "title+='\\nbigrams with frequency cut-off 10'\n",
    "xlab='regularization (logistic regression C parameter)'\n",
    "classification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n",
    "\n",
    "##  bigrams_cut_off_40\n",
    "print('bigrams_cut_off_40\\n')\n",
    "X_all=X_bigrams_cut_off_40\n",
    "filePath=plots_path+'logistic_bigrams_cut_off_40_translated3500Clusters.png'\n",
    "title='Logistic Regression'\n",
    "title+=' 5-fold cross-validation'\n",
    "title+='\\nbigrams with frequency cut-off 40'\n",
    "xlab='regularization (logistic regression C parameter)'\n",
    "classification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n",
    "\n",
    "\n",
    "##  bigrams_cut_off_200\n",
    "print('bigrams_cut_off_200\\n')\n",
    "X_all=X_bigrams_cut_off_200\n",
    "filePath=plots_path+'logistic_bigrams_cut_off_200_translated3500Clusters.png'\n",
    "title='Logistic Regression'\n",
    "title+=' 5-fold cross-validation'\n",
    "title+='\\nbigrams with frequency cut-off 200'\n",
    "xlab='regularization (logistic regression C parameter)'\n",
    "classification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)\n",
    "\n",
    "\n",
    "\n",
    "##  trigrams_cut_off_10\n",
    "print('trigrams_cut_off_10\\n')\n",
    "X_all=X_trigrams_cut_off_10\n",
    "filePath=plots_path+'logistic_trigrams_cut_off_10_translated3500Clusters.png'\n",
    "title='Logistic Regression'\n",
    "title+=' 5-fold cross-validation'\n",
    "title+='\\ntrigrams with frequency cut-off 10'\n",
    "xlab='regularization (logistic regression C parameter)'\n",
    "classification_functions.checkModel(clf,params,X_all,y,train_len,title,xlab,filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='l2', dual=False, tol=0.0001, \n",
    "                         C=1, fit_intercept=True, intercept_scaling=1.0, \n",
    "                         class_weight=None, random_state=None)\n",
    "\n",
    "clf.set_params(C=100)\n",
    "X_all=X_bigrams_cut_off_2\n",
    "X_all_train=X_all[:train_len]  \n",
    "X_all_test=X_all[train_len:]  \n",
    "\n",
    "\n",
    "clf.fit(X_all_train,y)\n",
    "\n",
    "predicted=clf.predict_proba(X_all_test)[:,1]\n",
    "\n",
    "inputFName=saved_data_path+'testSet.pkl'\n",
    "with open(inputFName,'rb') as fp:\n",
    "    test_current=cPickle.load(fp)\n",
    "\n",
    "test_current['sentiment']=predicted\n",
    "test_current['id']=test_current['id'].map(lambda x:x.replace('\"', '').strip())\n",
    "test_current=test_current[['id','sentiment']]\n",
    "test_current.to_csv(saved_data_path+'logisticRegression_bigrams_cut_off_2__translated3500Clusters_submission.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
